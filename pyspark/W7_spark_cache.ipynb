{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb1cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    " import getpass\n",
    " username = getpass.getuser()\n",
    " spark= SparkSession. \\\n",
    " builder. \\\n",
    " config('spark.ui.port','0'). \\\n",
    " config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    " enableHiveSupport(). \\\n",
    " master('yarn'). \\\n",
    " getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8710efab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:39961\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f77715f75c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49fceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv005857 supergroup      2.2 G 2023-06-06 09:28 /public/trendytech/datasets/cust_transf.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /public/trendytech/datasets/*cust_transf.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599d69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = (\"customer_id string, purchase_date date, product_id integer , amount float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd41198",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = spark.read\\\n",
    ".format(\"csv\") \\\n",
    ".schema(schema) \\\n",
    ".load(\"/public/trendytech/datasets/cust_transf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942b40c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "827527e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b74c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+------+\n",
      "|customer_id|purchase_date|product_id|amount|\n",
      "+-----------+-------------+----------+------+\n",
      "|       1001|   2023-05-15|      1001| 49.99|\n",
      "|       1002|   2023-05-16|      1002| 29.99|\n",
      "|       1003|   2023-05-17|      1003| 39.99|\n",
      "|       1004|   2023-05-18|      1004| 19.99|\n",
      "|       1005|   2023-05-19|      1005| 24.99|\n",
      "|       1001|   2023-05-20|      1002| 29.99|\n",
      "|       1002|   2023-05-21|      1003| 39.99|\n",
      "|       1003|   2023-05-22|      1004| 19.99|\n",
      "|       1004|   2023-05-23|      1005| 24.99|\n",
      "|       1005|   2023-05-24|      1001| 49.99|\n",
      "|       1001|   2023-05-25|      1003| 39.99|\n",
      "|       1002|   2023-05-26|      1004| 19.99|\n",
      "|       1003|   2023-05-27|      1005| 24.99|\n",
      "|       1004|   2023-05-28|      1001| 49.99|\n",
      "|       1005|   2023-05-29|      1002| 29.99|\n",
      "|       1001|   2023-05-30|      1003| 39.99|\n",
      "|       1002|   2023-05-31|      1004| 19.99|\n",
      "|       1003|   2023-06-01|      1005| 24.99|\n",
      "|       1004|   2023-06-02|      1001| 49.99|\n",
      "|       1005|   2023-06-03|      1002| 29.99|\n",
      "+-----------+-------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623d3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exampe of caching with simple example\n",
    "cached_df = main_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c992a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+------+\n",
      "|customer_id|purchase_date|product_id|amount|\n",
      "+-----------+-------------+----------+------+\n",
      "|       1001|   2023-05-15|      1001| 49.99|\n",
      "|       1002|   2023-05-16|      1002| 29.99|\n",
      "|       1003|   2023-05-17|      1003| 39.99|\n",
      "|       1004|   2023-05-18|      1004| 19.99|\n",
      "|       1005|   2023-05-19|      1005| 24.99|\n",
      "|       1001|   2023-05-20|      1002| 29.99|\n",
      "|       1002|   2023-05-21|      1003| 39.99|\n",
      "|       1003|   2023-05-22|      1004| 19.99|\n",
      "|       1004|   2023-05-23|      1005| 24.99|\n",
      "|       1005|   2023-05-24|      1001| 49.99|\n",
      "|       1001|   2023-05-25|      1003| 39.99|\n",
      "|       1002|   2023-05-26|      1004| 19.99|\n",
      "|       1003|   2023-05-27|      1005| 24.99|\n",
      "|       1004|   2023-05-28|      1001| 49.99|\n",
      "|       1005|   2023-05-29|      1002| 29.99|\n",
      "|       1001|   2023-05-30|      1003| 39.99|\n",
      "|       1002|   2023-05-31|      1004| 19.99|\n",
      "|       1003|   2023-06-01|      1005| 24.99|\n",
      "|       1004|   2023-06-02|      1001| 49.99|\n",
      "|       1005|   2023-06-03|      1002| 29.99|\n",
      "+-----------+-------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cached_df.show() # only partition will be cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d783a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_df.count() \n",
    "#since show fecthes first 20 rows , 1 partition will be cached\n",
    "# time taken - 30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0a4f2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87498290"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_df.count()  \n",
    "# time take - 0.5 sec -once its cached we see immense performance gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "348a934a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_id</th><th>purchase_date</th><th>product_id</th><th>amount</th></tr>\n",
       "<tr><td>1001</td><td>2023-05-15</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-16</td><td>1002</td><td>29.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-05-17</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-05-18</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-05-19</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1001</td><td>2023-05-20</td><td>1002</td><td>29.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-21</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-05-22</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-05-23</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-05-24</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1001</td><td>2023-05-25</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-26</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-05-27</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-05-28</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-05-29</td><td>1002</td><td>29.99</td></tr>\n",
       "<tr><td>1001</td><td>2023-05-30</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-31</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-06-01</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-06-02</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-06-03</td><td>1002</td><td>29.99</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+-------------+----------+------+\n",
       "|customer_id|purchase_date|product_id|amount|\n",
       "+-----------+-------------+----------+------+\n",
       "|       1001|   2023-05-15|      1001| 49.99|\n",
       "|       1002|   2023-05-16|      1002| 29.99|\n",
       "|       1003|   2023-05-17|      1003| 39.99|\n",
       "|       1004|   2023-05-18|      1004| 19.99|\n",
       "|       1005|   2023-05-19|      1005| 24.99|\n",
       "|       1001|   2023-05-20|      1002| 29.99|\n",
       "|       1002|   2023-05-21|      1003| 39.99|\n",
       "|       1003|   2023-05-22|      1004| 19.99|\n",
       "|       1004|   2023-05-23|      1005| 24.99|\n",
       "|       1005|   2023-05-24|      1001| 49.99|\n",
       "|       1001|   2023-05-25|      1003| 39.99|\n",
       "|       1002|   2023-05-26|      1004| 19.99|\n",
       "|       1003|   2023-05-27|      1005| 24.99|\n",
       "|       1004|   2023-05-28|      1001| 49.99|\n",
       "|       1005|   2023-05-29|      1002| 29.99|\n",
       "|       1001|   2023-05-30|      1003| 39.99|\n",
       "|       1002|   2023-05-31|      1004| 19.99|\n",
       "|       1003|   2023-06-01|      1005| 24.99|\n",
       "|       1004|   2023-06-02|      1001| 49.99|\n",
       "|       1005|   2023-06-03|      1002| 29.99|\n",
       "+-----------+-------------+----------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_df.unpersist()  # releasing the cached data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b39648f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Design a caching strategy that efficiently retrieves the top-selling products by revenue\n",
    "#  Demonstrate the impact of caching by comparing the retrieval time for Top 10 best-selling products from start_date = \"2023-05-01\" to\n",
    "#  end_date = \"2023-06-08\" before and after implementing the caching strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9297b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With OUT caching\n",
    "start_date = \"2023-05-01\"\n",
    "end_date = \"2023-06-08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14285075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = main_df.filter((main_df.purchase_date >=start_date) & (main_df.purchase_date <=end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af6f3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57d53da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df_filtered.groupBy('product_id').sum('amount').withColumnRenamed('sum(amount)', 'revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8372dd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|product_id|             revenue|\n",
      "+----------+--------------------+\n",
      "|      1003| 5.725592484315491E8|\n",
      "|      1001| 5.566826598912048E8|\n",
      "|      1002| 4.293836211229706E8|\n",
      "|      1004| 2.862080211229706E8|\n",
      "|      1005|2.7828563865119934E8|\n",
      "|      1015|   12537.91035079956|\n",
      "|      1014|   11492.91035079956|\n",
      "|      1013|   10447.91035079956|\n",
      "|      1012|    9402.91035079956|\n",
      "|      1011|    8357.91035079956|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_10=agg_df.sort('revenue',ascending=False).limit(10).show()\n",
    "# time taken - 20 secods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e9e9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage1 -  18 partitions | local aggreagtion based on 'product_id' | written to disk\n",
    "filtered_date  = main_df.filter(((main_df.purchase_date)>= start_date) & ((main_df.purchase_date)<=end_date)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85018931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage2 - wide transformation | 200 partitions | Similar product_id will go to same partitions | perform final aggreagtion | write to disk\n",
    "agg_df = filtered_date.groupBy('product_id').sum('amount').withColumnRenamed(\"sum(amount)\",'revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f723e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|product_id|             revenue|\n",
      "+----------+--------------------+\n",
      "|      1003| 5.725592484315491E8|\n",
      "|      1001| 5.566826598912048E8|\n",
      "|      1002| 4.293836211229706E8|\n",
      "|      1004| 2.862080211229706E8|\n",
      "|      1005|2.7828563865119934E8|\n",
      "|      1015|   12537.91035079956|\n",
      "|      1014|   11492.91035079956|\n",
      "|      1013|   10447.91035079956|\n",
      "|      1012|    9402.91035079956|\n",
      "|      1011|    8357.91035079956|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stage3 - Read the fianl aggregated results and sort \n",
    "agg_df.sort(\"revenue\",ascending = False).limit(10).show()\n",
    "# intially it took 40 sec , when ran the query again it ran in 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dff57088",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_id</th><th>purchase_date</th><th>product_id</th><th>amount</th></tr>\n",
       "<tr><td>1001</td><td>2023-05-15</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-16</td><td>1002</td><td>29.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-05-17</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-05-18</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-05-19</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1001</td><td>2023-05-20</td><td>1002</td><td>29.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-21</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-05-22</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-05-23</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-05-24</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1001</td><td>2023-05-25</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-26</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-05-27</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-05-28</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-05-29</td><td>1002</td><td>29.99</td></tr>\n",
       "<tr><td>1001</td><td>2023-05-30</td><td>1003</td><td>39.99</td></tr>\n",
       "<tr><td>1002</td><td>2023-05-31</td><td>1004</td><td>19.99</td></tr>\n",
       "<tr><td>1003</td><td>2023-06-01</td><td>1005</td><td>24.99</td></tr>\n",
       "<tr><td>1004</td><td>2023-06-02</td><td>1001</td><td>49.99</td></tr>\n",
       "<tr><td>1005</td><td>2023-06-03</td><td>1002</td><td>29.99</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+-------------+----------+------+\n",
       "|customer_id|purchase_date|product_id|amount|\n",
       "+-----------+-------------+----------+------+\n",
       "|       1001|   2023-05-15|      1001| 49.99|\n",
       "|       1002|   2023-05-16|      1002| 29.99|\n",
       "|       1003|   2023-05-17|      1003| 39.99|\n",
       "|       1004|   2023-05-18|      1004| 19.99|\n",
       "|       1005|   2023-05-19|      1005| 24.99|\n",
       "|       1001|   2023-05-20|      1002| 29.99|\n",
       "|       1002|   2023-05-21|      1003| 39.99|\n",
       "|       1003|   2023-05-22|      1004| 19.99|\n",
       "|       1004|   2023-05-23|      1005| 24.99|\n",
       "|       1005|   2023-05-24|      1001| 49.99|\n",
       "|       1001|   2023-05-25|      1003| 39.99|\n",
       "|       1002|   2023-05-26|      1004| 19.99|\n",
       "|       1003|   2023-05-27|      1005| 24.99|\n",
       "|       1004|   2023-05-28|      1001| 49.99|\n",
       "|       1005|   2023-05-29|      1002| 29.99|\n",
       "|       1001|   2023-05-30|      1003| 39.99|\n",
       "|       1002|   2023-05-31|      1004| 19.99|\n",
       "|       1003|   2023-06-01|      1005| 24.99|\n",
       "|       1004|   2023-06-02|      1001| 49.99|\n",
       "|       1005|   2023-06-03|      1002| 29.99|\n",
       "+-----------+-------------+----------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.stop()\n",
    "filtered_date.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c0f56",
   "metadata": {},
   "source": [
    "2) Write a query to fetch the total count of hotel bookings in the\n",
    " hotel_bookings table and compare the duration it took to determine the impact of caching.\n",
    "  Design a caching mechanism using spark external tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3507f058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv005857 supergroup      5.6 K 2023-06-05 02:31 /public/trendytech/datasets/hotel_data.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /public/trendytech/datasets/hotel_data.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bf324",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database itv011856\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "278f26a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    namespace|\n",
      "+-------------+\n",
      "|    itv011856|\n",
      "|w5m_itv011856|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').filter(\"namespace like '%itv011856'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating external spark table\n",
    "spark.sql(\"create table itv011856.hotels(booking_id int, guest_name string, checkin_date date, checkout_date date, room_type string, total_price float)using csv location '/public/trendytech/datasets/hotel_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f553f6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>107</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|     107|\n",
       "+--------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from itv011856.hotels \")\n",
    "# took 1.25 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a15ca68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caching the externl table. caching with spark tables is NOT Lazy unlike dataframe were it's Lazy\n",
    "spark.sql(\"cache table itv011856.hotels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e64982a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>107</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|     107|\n",
       "+--------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from itv011856.hotels \")\n",
    "# less then 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64ddb653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"uncache table itv011856.hotels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61430939",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
