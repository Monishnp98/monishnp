{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1e1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    " import getpass\n",
    " username = getpass.getuser()\n",
    " spark= SparkSession. \\\n",
    " builder. \\\n",
    " config('spark.ui.port','0'). \\\n",
    " config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    " enableHiveSupport(). \\\n",
    " master('yarn'). \\\n",
    " getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da69c78",
   "metadata": {},
   "source": [
    "# CONTENTS COVERED\n",
    "    # Enforcing schema\n",
    "    # Different types of creating df (df from test data,reader api, .sql...ect)\n",
    "    # Handling date fields\n",
    "    # Expression handling\n",
    "    # Drop columns & drop dup row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5e8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write PySpark code to create a new dataframe with the data given below\n",
    "#  having 2 columns (‘season’) and (‘windspeed’).\n",
    "#  # Data\n",
    "#  [(\"Spring\", 12.3),\n",
    "#  (\"Summer\", 10.5),\n",
    "#  (\"Autumn\", 8.2),\n",
    "#  (\"Winter\", 15.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ec3431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|season|windspeed|\n",
      "+------+---------+\n",
      "|Spring|     12.3|\n",
      "|Summer|     10.5|\n",
      "|Autumn|      8.2|\n",
      "|Winter|     15.1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tst_data = [(\"Spring\", 12.3), (\"Summer\", 10.5),(\"Autumn\", 8.2),(\"Winter\", 15.1)]\n",
    "sctructure = ('season string,windspeed float')\n",
    "season_df = spark.createDataFrame(tst_data,sctructure)\n",
    "season_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221c0d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- windspeed: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "season_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8666022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|    _1|  _2|\n",
      "+------+----+\n",
      "|Spring|12.3|\n",
      "|Summer|10.5|\n",
      "|Autumn| 8.2|\n",
      "|Winter|15.1|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If schema is infered , will not have col names\n",
    "season_df_tst = spark.createDataFrame(tst_data)\n",
    "season_df_tst.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506fd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)  load  data into a Dataframe and enforce schema using StructType from  \"/public/trendytech/datasets/library_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bac5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"library_name\": \"Central Library\",\"location\": \"City Center\",\"books\": [{\"book_id\": \"B001\",\"book_name\": \"The Great Gatsby\",\"author\": \"F. Scott Fitzgerald\",\"copies_available\": 5},{\"book_id\": \"B002\",\"book_name\": \"To Kill a Mockingbird\",\"author\": \"Harper Lee\",\"copies_available\": 3}],\"members\": [{\"member_id\": \"M001\",\"member_name\": \"John Smith\",\"age\": 28,\"books_borrowed\": [\"B001\"]},{\"member_id\": \"M002\",\"member_name\": \"Emma Johnson\",\"age\": 35,\"books_borrowed\": []}]},\n",
      "{\"library_name\": \"Community Library\",\"location\": \"Suburb\",\"books\": [{\"book_id\": \"B003\",\"book_name\": \"1984\",\"author\": \"George Orwell\",\"copies_available\": 2},{\"book_id\": \"B004\",\"book_name\": \"Pride and Prejudice\",\"author\": \"Jane Austen\",\"copies_available\": 4}],\"members\": [{\"member_id\": \"M003\",\"member_name\": \"Michael Brown\",\"age\": 42,\"books_borrowed\": [\"B003\",\"B004\"]},{\"member_id\": \"M004\",\"member_name\": \"Sophia Davis\",\"age\": 31,\"books_borrowed\": [\"B004\"]}]}\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -head /public/trendytech/datasets/library_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bde15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NESTED SCHEMA\n",
    "# {\"library_name\": \"Central Library\",\"location\": \"City Center\",\n",
    " \n",
    "#  \"books\": [{\"book_id\": \"B001\",\"book_name\": \"The Great Gatsby\",\"author\": \"F. Scott Fitzgerald\",\"copies_available\": 5},\n",
    "#           {\"book_id\": \"B002\",\"book_name\": \"To Kill a Mockingbird\",\"author\": \"Harper Lee\",\"copies_available\": 3}],\n",
    " \n",
    "#  \"members\": [{\"member_id\": \"M001\",\"member_name\": \"John Smith\",\"age\": 28,\"books_borrowed\": [\"B001\"]},\n",
    "#              {\"member_id\": \"M002\",\"member_name\": \"Emma Johnson\",\"age\": 35,\"books_borrowed\": []}]\n",
    "# },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd24dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820275d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_schema = StructType([\n",
    "StructField(\"library_name\",StringType()),\n",
    "StructField(\"location\",StringType()),\n",
    "StructField(\"books\",ArrayType(StructType([StructField(\"book_id\",StringType()),StructField(\"book_name\",StringType()),StructField(\"author\",StringType()),StructField(\"copies_available\",IntegerType())]))),\n",
    "StructField(\"members\",ArrayType(StructType([StructField(\"member_id\",StringType()),StructField(\"member_name\",StringType()),StructField(\"age\",IntegerType()),StructField(\"books_borrowed\",StringType())])))\n",
    "\t])\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e593e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = spark.read \\\n",
    ".format('json') \\\n",
    ".schema(book_schema) \\\n",
    ".load('/public/trendytech/datasets/library_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab97830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- library_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- books: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- book_id: string (nullable = true)\n",
      " |    |    |-- book_name: string (nullable = true)\n",
      " |    |    |-- author: string (nullable = true)\n",
      " |    |    |-- copies_available: integer (nullable = true)\n",
      " |-- members: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- member_id: string (nullable = true)\n",
      " |    |    |-- member_name: string (nullable = true)\n",
      " |    |    |-- age: integer (nullable = true)\n",
      " |    |    |-- books_borrowed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# book_df.show()\n",
    "book_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a21803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Given the dataset (/public/trendytech/datasets/train.csv),create a Dataframe using PySpark and perform the following operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f99e88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_number,train_name,seats_available,passenger_name,age,ticket_number,seat_number\n",
      "123,Express,100,John,25,T123,A1\n",
      "123,Express,100,Emma,30,T124,B2\n",
      "456,Superfast,150,Michael,35,T125,C3\n",
      "456,Superfast,150,Sophia,40,T126,D4\n",
      "789,Local,50,William,28,T127,E5\n",
      "789,Local,50,Sophia,32,T128,F6\n",
      "789,Local,50,Oliver,45,T129,G7\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -head /public/trendytech/datasets/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef39890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"inferSchema\",True)\\\n",
    ".option(\"header\",True)\\\n",
    ".load(\"/public/trendytech/datasets/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979d2fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|train_number|train_name|seats_available|passenger_name|age|ticket_number|seat_number|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|         123|   Express|            100|          John| 25|         T123|         A1|\n",
      "|         123|   Express|            100|          Emma| 30|         T124|         B2|\n",
      "|         456| Superfast|            150|       Michael| 35|         T125|         C3|\n",
      "|         456| Superfast|            150|        Sophia| 40|         T126|         D4|\n",
      "|         789|     Local|             50|       William| 28|         T127|         E5|\n",
      "|         789|     Local|             50|        Sophia| 32|         T128|         F6|\n",
      "|         789|     Local|             50|        Oliver| 45|         T129|         G7|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00e5fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 a) Drop the columns passenger_name and age from the dataset.\n",
    "tarin_col_drop = train_df.drop(\"passenger_name\" ,\"age\")\n",
    "tarin_col_drop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "612df30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 b) Count the number of rows after removing duplicates of columns train_number and ticket_number\n",
    "train_dups = train_df.dropDuplicates(['train_number','ticket_number'])\n",
    "train_dups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2c03d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 c) Count the number of unique train names\n",
    "train_df.select('train_name').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a9e731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Using PySpark, load the data into a Dataframe and perform the following operations on the hospital dataset (/public/trendytech/datasets/hospital.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f17b926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id,admission_date,discharge_date,diagnosis,doctor_id,total_cost\n",
      "1,01-01-2022,2022-01-10,Pneumonia,101,5000.00\n",
      "2,02-05-2022,2022-02-09,Appendicitis,102,7000.00\n",
      "3,03-12-2022,2022-03-18,Fractured Arm,103,3500.00\n",
      "4,04-02-2022,2022-04-08,Heart Attack,104,15000.00\n",
      "5,05-05-2022,2022-05-07,Influenza,105,2500.00\n",
      "6,06-10-2022,2022-06-15,Appendicitis,106,8000.00\n",
      "7,07-20-2022,2022-07-25,Pneumonia,107,5500.00\n",
      "8,08-25-2022,2022-09-01,Heart Attack,108,20000.00\n",
      "9,09-15-2022,2022-09-22,Fractured Leg,109,6000.00\n",
      "10,10-05-2022,2022-10-10,Appendicitis,110,7500.00\n",
      "11,11-02-2022,2022-11-05,Influenza,111,2800.00\n",
      "12,12-10-2022,2022-12-18,Pneumonia,112,6000.00\n",
      "13,01-02-2023,2023-01-09,Heart Attack,113,18000.00\n",
      "14,02-14-2023,2023-02-18,Appendicitis,114,7200.00\n",
      "15,03-20-2023,2023-03-28,Fractured Arm,115,3800.00\n",
      "16,04-05-2023,2023-04-11,Influenza,116,2700.00\n",
      "17,05-08-2023,2023-05-11,Heart Attack,117,16000.00\n",
      "18,06-15-2023,2023-06-20,Pneumonia,118,4800.00\n",
      "19,07-22-2023,2023-07-27,Fractured Leg,119,6500.00\n",
      "20,0"
     ]
    }
   ],
   "source": [
    "! hadoop fs -head /public/trendytech/datasets/hospital.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdb6862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df =  spark.read \\\n",
    ".format('csv') \\\n",
    ".option(\"header\",True) \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".load(\"/public/trendytech/datasets/hospital.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47a73093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- admission_date: string (nullable = true)\n",
      " |-- discharge_date: string (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- doctor_id: integer (nullable = true)\n",
      " |-- total_cost: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df.printSchema()\n",
    "# Date fields are loaded as null , will use to_date and convert to date in the next steps\n",
    "# or while loading the df we can use .option(\"dateFormat\",'yyy-mm-dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae804537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|total_cost|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|    5000.0|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|    7000.0|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|    3500.0|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|   15000.0|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|    2500.0|\n",
      "|         6|    06-10-2022|    2022-06-15| Appendicitis|    8000.0|\n",
      "|         7|    07-20-2022|    2022-07-25|    Pneumonia|    5500.0|\n",
      "|         8|    08-25-2022|    2022-09-01| Heart Attack|   20000.0|\n",
      "|         9|    09-15-2022|    2022-09-22|Fractured Leg|    6000.0|\n",
      "|        10|    10-05-2022|    2022-10-10| Appendicitis|    7500.0|\n",
      "|        11|    11-02-2022|    2022-11-05|    Influenza|    2800.0|\n",
      "|        12|    12-10-2022|    2022-12-18|    Pneumonia|    6000.0|\n",
      "|        13|    01-02-2023|    2023-01-09| Heart Attack|   18000.0|\n",
      "|        14|    02-14-2023|    2023-02-18| Appendicitis|    7200.0|\n",
      "|        15|    03-20-2023|    2023-03-28|Fractured Arm|    3800.0|\n",
      "|        16|    04-05-2023|    2023-04-11|    Influenza|    2700.0|\n",
      "|        17|    05-08-2023|    2023-05-11| Heart Attack|   16000.0|\n",
      "|        18|    06-15-2023|    2023-06-20|    Pneumonia|    4800.0|\n",
      "|        19|    07-22-2023|    2023-07-27|Fractured Leg|    6500.0|\n",
      "|        20|    08-10-2023|    2023-08-16| Appendicitis|    7800.0|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 a) Drop the \"doctor_id\" column from the dataset.\n",
    "hospital_df = hospital_df.drop('doctor_id')\n",
    "hospital_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4300fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|       5000.0|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|       7000.0|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|       3500.0|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|      15000.0|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|       2500.0|\n",
      "|         6|    06-10-2022|    2022-06-15| Appendicitis|       8000.0|\n",
      "|         7|    07-20-2022|    2022-07-25|    Pneumonia|       5500.0|\n",
      "|         8|    08-25-2022|    2022-09-01| Heart Attack|      20000.0|\n",
      "|         9|    09-15-2022|    2022-09-22|Fractured Leg|       6000.0|\n",
      "|        10|    10-05-2022|    2022-10-10| Appendicitis|       7500.0|\n",
      "|        11|    11-02-2022|    2022-11-05|    Influenza|       2800.0|\n",
      "|        12|    12-10-2022|    2022-12-18|    Pneumonia|       6000.0|\n",
      "|        13|    01-02-2023|    2023-01-09| Heart Attack|      18000.0|\n",
      "|        14|    02-14-2023|    2023-02-18| Appendicitis|       7200.0|\n",
      "|        15|    03-20-2023|    2023-03-28|Fractured Arm|       3800.0|\n",
      "|        16|    04-05-2023|    2023-04-11|    Influenza|       2700.0|\n",
      "|        17|    05-08-2023|    2023-05-11| Heart Attack|      16000.0|\n",
      "|        18|    06-15-2023|    2023-06-20|    Pneumonia|       4800.0|\n",
      "|        19|    07-22-2023|    2023-07-27|Fractured Leg|       6500.0|\n",
      "|        20|    08-10-2023|    2023-08-16| Appendicitis|       7800.0|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 .b) Rename the \"total_cost\" column to \"hospital_bill\"\n",
    "hospital_df  = hospital_df.withColumnRenamed('total_cost','hospital_bill')\n",
    "hospital_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e772d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 c). Add a new column called \"duration_of_stay\" that represents the number of days a patient stayed in the hospital. \n",
    "    # (hint: The duration should be calculated as the difference between the \"discharge_date\" and \"admission_date\" columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39cc31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c368ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df = hospital_df.withColumn('admission_date',to_date('admission_date','dd-mm-yyyy'))\n",
    "hospital_df = hospital_df.withColumn('discharge_date',to_date('discharge_date','yyyy-mm-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f68d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- admission_date: date (nullable = true)\n",
      " |-- discharge_date: date (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- hospital_bill: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df.printSchema()\n",
    "# admission_date & discharge_date are convered to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37b359a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|\n",
      "|         2|    2022-01-02|    2022-01-09| Appendicitis|       7000.0|               7|\n",
      "|         3|    2022-01-03|    2022-01-18|Fractured Arm|       3500.0|              15|\n",
      "|         4|    2022-01-04|    2022-01-08| Heart Attack|      15000.0|               4|\n",
      "|         5|    2022-01-05|    2022-01-07|    Influenza|       2500.0|               2|\n",
      "|         6|    2022-01-06|    2022-01-15| Appendicitis|       8000.0|               9|\n",
      "|         7|    2022-01-07|    2022-01-25|    Pneumonia|       5500.0|              18|\n",
      "|         8|    2022-01-08|    2022-01-01| Heart Attack|      20000.0|              -7|\n",
      "|         9|    2022-01-09|    2022-01-22|Fractured Leg|       6000.0|              13|\n",
      "|        10|    2022-01-10|    2022-01-10| Appendicitis|       7500.0|               0|\n",
      "|        11|    2022-01-11|    2022-01-05|    Influenza|       2800.0|              -6|\n",
      "|        12|    2022-01-12|    2022-01-18|    Pneumonia|       6000.0|               6|\n",
      "|        13|    2023-01-01|    2023-01-09| Heart Attack|      18000.0|               8|\n",
      "|        14|    2023-01-02|    2023-01-18| Appendicitis|       7200.0|              16|\n",
      "|        15|    2023-01-03|    2023-01-28|Fractured Arm|       3800.0|              25|\n",
      "|        16|    2023-01-04|    2023-01-11|    Influenza|       2700.0|               7|\n",
      "|        17|    2023-01-05|    2023-01-11| Heart Attack|      16000.0|               6|\n",
      "|        18|    2023-01-06|    2023-01-20|    Pneumonia|       4800.0|              14|\n",
      "|        19|    2023-01-07|    2023-01-27|Fractured Leg|       6500.0|              20|\n",
      "|        20|    2023-01-08|    2023-01-16| Appendicitis|       7800.0|               8|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df = hospital_df.withColumn('duration_of_stay',expr(\"dateDiff(discharge_date,admission_date)\"))\n",
    "hospital_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "208541ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  4 d) Create a new column called \"adjusted_total_cost\" that calculates the adjusted total cost based on the diagnosis as follows:\n",
    "        #  If the diagnosis is \"Heart Attack\", multiply the hospital_bill by 1.5.\n",
    "        #  If the diagnosis is \"Appendicitis\", multiply the hospital_bill by 1.2.\n",
    "        #  For any other diagnosis, keep the hospital_bill as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "520f4af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|adjusted_total_cost|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|             5000.0|\n",
      "|         2|    2022-01-02|    2022-01-09| Appendicitis|       7000.0|               7|             8400.0|\n",
      "|         3|    2022-01-03|    2022-01-18|Fractured Arm|       3500.0|              15|             3500.0|\n",
      "|         4|    2022-01-04|    2022-01-08| Heart Attack|      15000.0|               4|            22500.0|\n",
      "|         5|    2022-01-05|    2022-01-07|    Influenza|       2500.0|               2|             2500.0|\n",
      "|         6|    2022-01-06|    2022-01-15| Appendicitis|       8000.0|               9|             9600.0|\n",
      "|         7|    2022-01-07|    2022-01-25|    Pneumonia|       5500.0|              18|             5500.0|\n",
      "|         8|    2022-01-08|    2022-01-01| Heart Attack|      20000.0|              -7|            30000.0|\n",
      "|         9|    2022-01-09|    2022-01-22|Fractured Leg|       6000.0|              13|             6000.0|\n",
      "|        10|    2022-01-10|    2022-01-10| Appendicitis|       7500.0|               0|             9000.0|\n",
      "|        11|    2022-01-11|    2022-01-05|    Influenza|       2800.0|              -6|             2800.0|\n",
      "|        12|    2022-01-12|    2022-01-18|    Pneumonia|       6000.0|               6|             6000.0|\n",
      "|        13|    2023-01-01|    2023-01-09| Heart Attack|      18000.0|               8|            27000.0|\n",
      "|        14|    2023-01-02|    2023-01-18| Appendicitis|       7200.0|              16|             8640.0|\n",
      "|        15|    2023-01-03|    2023-01-28|Fractured Arm|       3800.0|              25|             3800.0|\n",
      "|        16|    2023-01-04|    2023-01-11|    Influenza|       2700.0|               7|             2700.0|\n",
      "|        17|    2023-01-05|    2023-01-11| Heart Attack|      16000.0|               6|            24000.0|\n",
      "|        18|    2023-01-06|    2023-01-20|    Pneumonia|       4800.0|              14|             4800.0|\n",
      "|        19|    2023-01-07|    2023-01-27|Fractured Leg|       6500.0|              20|             6500.0|\n",
      "|        20|    2023-01-08|    2023-01-16| Appendicitis|       7800.0|               8|             9360.0|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hospital_prefinal = hospital_df.withColumn('adjusted_total_cost',expr(\"CASE WHEN diagnosis LIKE '%Heart Attack%' THEN  hospital_bill * 1.5 WHEN diagnosis LIKE '%Appendicitis%' THEN  hospital_bill * 1.2 ELSE hospital_bill END\"))\n",
    "Hospital_prefinal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de70245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|adjusted_total_cost|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|             5000.0|\n",
      "|         2|    2022-01-02|    2022-01-09| Appendicitis|       7000.0|               7|             8400.0|\n",
      "|         3|    2022-01-03|    2022-01-18|Fractured Arm|       3500.0|              15|             3500.0|\n",
      "|         4|    2022-01-04|    2022-01-08| Heart Attack|      15000.0|               4|            22500.0|\n",
      "|         5|    2022-01-05|    2022-01-07|    Influenza|       2500.0|               2|             2500.0|\n",
      "|         6|    2022-01-06|    2022-01-15| Appendicitis|       8000.0|               9|             9600.0|\n",
      "|         7|    2022-01-07|    2022-01-25|    Pneumonia|       5500.0|              18|             5500.0|\n",
      "|         8|    2022-01-08|    2022-01-01| Heart Attack|      20000.0|              -7|            30000.0|\n",
      "|         9|    2022-01-09|    2022-01-22|Fractured Leg|       6000.0|              13|             6000.0|\n",
      "|        10|    2022-01-10|    2022-01-10| Appendicitis|       7500.0|               0|             9000.0|\n",
      "|        11|    2022-01-11|    2022-01-05|    Influenza|       2800.0|              -6|             2800.0|\n",
      "|        12|    2022-01-12|    2022-01-18|    Pneumonia|       6000.0|               6|             6000.0|\n",
      "|        13|    2023-01-01|    2023-01-09| Heart Attack|      18000.0|               8|            27000.0|\n",
      "|        14|    2023-01-02|    2023-01-18| Appendicitis|       7200.0|              16|             8640.0|\n",
      "|        15|    2023-01-03|    2023-01-28|Fractured Arm|       3800.0|              25|             3800.0|\n",
      "|        16|    2023-01-04|    2023-01-11|    Influenza|       2700.0|               7|             2700.0|\n",
      "|        17|    2023-01-05|    2023-01-11| Heart Attack|      16000.0|               6|            24000.0|\n",
      "|        18|    2023-01-06|    2023-01-20|    Pneumonia|       4800.0|              14|             4800.0|\n",
      "|        19|    2023-01-07|    2023-01-27|Fractured Leg|       6500.0|              20|             6500.0|\n",
      "|        20|    2023-01-08|    2023-01-16| Appendicitis|       7800.0|               8|             9360.0|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hospital_prefinal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7b473cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 e)Select the \"patient_id\", \"diagnosis\", \"hospital_bill\", and  \"adjusted_total_cost\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ee0a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+-------------------+\n",
      "|patient_id|    diagnosis|hospital_bill|adjusted_total_cost|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "|         1|    Pneumonia|       5000.0|             5000.0|\n",
      "|         2| Appendicitis|       7000.0|             8400.0|\n",
      "|         3|Fractured Arm|       3500.0|             3500.0|\n",
      "|         4| Heart Attack|      15000.0|            22500.0|\n",
      "|         5|    Influenza|       2500.0|             2500.0|\n",
      "|         6| Appendicitis|       8000.0|             9600.0|\n",
      "|         7|    Pneumonia|       5500.0|             5500.0|\n",
      "|         8| Heart Attack|      20000.0|            30000.0|\n",
      "|         9|Fractured Leg|       6000.0|             6000.0|\n",
      "|        10| Appendicitis|       7500.0|             9000.0|\n",
      "|        11|    Influenza|       2800.0|             2800.0|\n",
      "|        12|    Pneumonia|       6000.0|             6000.0|\n",
      "|        13| Heart Attack|      18000.0|            27000.0|\n",
      "|        14| Appendicitis|       7200.0|             8640.0|\n",
      "|        15|Fractured Arm|       3800.0|             3800.0|\n",
      "|        16|    Influenza|       2700.0|             2700.0|\n",
      "|        17| Heart Attack|      16000.0|            24000.0|\n",
      "|        18|    Pneumonia|       4800.0|             4800.0|\n",
      "|        19|Fractured Leg|       6500.0|             6500.0|\n",
      "|        20| Appendicitis|       7800.0|             9360.0|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hospital_final_df = Hospital_prefinal.select('patient_id','diagnosis','hospital_bill','adjusted_total_cost')\n",
    "Hospital_final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bf681fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
